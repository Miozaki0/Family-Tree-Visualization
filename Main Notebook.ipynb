{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qwikidata\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "API_endpoint = 'https://www.wikidata.org/w/api.php'\n",
    "keywords = pd.read_csv('files/keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Global cache for keywords to reduce repeated API calls\n",
    "keyword_cache = {}\n",
    "\n",
    "\n",
    "def search(query: str):\n",
    "    global API_endpoint\n",
    "    params = {\n",
    "        'action': 'wbsearchentities',\n",
    "        'format' : 'json',\n",
    "        'language': 'en',\n",
    "        'search' : query\n",
    "    }\n",
    "    \n",
    "    r = requests.get(API_endpoint, params = params)\n",
    "    for i in r.json()['search']:\n",
    "        yield str(i['id'])\n",
    "\n",
    "\n",
    "        \n",
    "def keyword(query: str):\n",
    "    \"\"\"Retrieve the label for a Wikidata entity by ID with caching.\"\"\"\n",
    "    global API_endpoint, keywords, keyword_cache\n",
    "    \n",
    "    # Check in local cache\n",
    "    if query in keyword_cache:\n",
    "        return keyword_cache[query]\n",
    "    \n",
    "    # Check in keywords DataFrame\n",
    "    if query in set(keywords['id']):\n",
    "        result = keywords.loc[keywords['id'] == query, 'name'].values[0]\n",
    "        keyword_cache[query] = result\n",
    "        return result\n",
    "    \n",
    "    # Fetch from API\n",
    "    params = {\n",
    "        'action': 'wbgetentities',\n",
    "        'format': 'json',\n",
    "        'language': 'en',\n",
    "        'ids': query\n",
    "    }\n",
    "    response = requests.get(API_endpoint, params=params)\n",
    "    output = response.json()['entities'][str(query)][\"labels\"]['en']['value']\n",
    "    \n",
    "    # Update cache and keywords DataFrame\n",
    "    keyword_cache[query] = output\n",
    "    new_df = pd.DataFrame({'id': [query], 'name': [output]})\n",
    "    keywords = pd.concat([keywords, new_df], ignore_index=True)\n",
    "    keywords.to_csv('files/keywords.csv', index=False)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def family(person):\n",
    "    \"\"\"Retrieve family details for a person.\"\"\"\n",
    "    global API_endpoint\n",
    "    entity_id = next(search(person))\n",
    "    \n",
    "    entity_params = {\n",
    "        'action': 'wbgetentities',\n",
    "        'format': 'json',\n",
    "        'language': 'en',\n",
    "        'ids': entity_id\n",
    "    }\n",
    "    \n",
    "    response = requests.get(API_endpoint, params=entity_params)\n",
    "    entity_data = response.json()\n",
    "    claims = entity_data['entities'][entity_id]['claims']\n",
    "    \n",
    "    # Pre-fetch keywords for occupations and citizenships\n",
    "    occupation_ids = [claim['mainsnak']['datavalue']['value']['id'] for claim in claims.get('P106', [])]\n",
    "    citizenship_ids = [claim['mainsnak']['datavalue']['value']['id'] for claim in claims.get('P27', [])]\n",
    "    \n",
    "    occupation = [keyword(i) for i in occupation_ids]\n",
    "    citizenships = [keyword(i) for i in citizenship_ids]\n",
    "    \n",
    "    # Retrieve personal information with defaults for missing fields\n",
    "    personal_info = {\n",
    "        \"name\": entity_data['entities'][entity_id]['labels'].get('en', {}).get('value', []),\n",
    "        \"id\": entity_id,\n",
    "        \"birth_date\": claims.get('P569', [{}])[0].get('mainsnak', {}).get('datavalue', {}).get('value', {}).get('time', \"Unknown\"),\n",
    "        \"birth_place\": claims.get('P19', [{}])[0].get('mainsnak', {}).get('datavalue', {}).get('value', {}).get('id', \"Unknown\"),\n",
    "        \"death_date\": claims.get('P570', [{}])[0].get('mainsnak', {}).get('datavalue', {}).get('value', {}).get('time', None),\n",
    "        \"death_place\": keyword(claims.get('P20', [{}])[0].get('mainsnak', {}).get('datavalue', {}).get('value', {}).get('id', \"Unknown\")) if claims.get('P20') else \"Unknown\",\n",
    "        \"gender\": claims.get('P21', [{}])[0].get('mainsnak', {}).get('datavalue', {}).get('value', {}).get('id'),\n",
    "        \"spouses\": [claim['mainsnak']['datavalue']['value']['id'] for claim in claims.get('P26', [])],\n",
    "        \"children\": [claim['mainsnak'].get('datavalue', {}).get('value', {}).get('id', {}) for claim in claims.get('P40', {})],\n",
    "        \"father\": claims.get('P22', [{}])[0].get('mainsnak', {}).get('datavalue', {}).get('value', {}).get('id'),\n",
    "        \"mother\": claims.get('P25', [{}])[0].get('mainsnak', {}).get('datavalue', {}).get('value', {}).get('id'),\n",
    "    }\n",
    "    \n",
    "    return personal_info\n",
    "\n",
    "def neighbours(info):\n",
    "    \"\"\"Retrieve family connections from personal info.\"\"\"\n",
    "    return list(filter(None, info['spouses'] + info['children'] + [info['father'], info['mother']]))\n",
    "\n",
    "\n",
    "def BFS(start_id, max_depth=2):\n",
    "    \"\"\"\n",
    "    Breadth-first search to explore family relationships with a specified depth limit.\n",
    "    \n",
    "    Args:\n",
    "        start_id (str): The ID of the starting person.\n",
    "        max_depth (int): Maximum depth to explore (default: infinite).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where each key is a person ID, and the value is their personal info,\n",
    "              including the distance from the starting node.\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    queue = deque([(start_id, 0)])  # Queue stores (current_node, depth)\n",
    "    output = {}\n",
    "    \n",
    "    while queue:\n",
    "        current_node, depth = queue.popleft()\n",
    "        #print(f\"Processing node {current_node} at depth {depth}\")\n",
    "        \n",
    "        if current_node in visited or depth > max_depth:\n",
    "            continue\n",
    "        \n",
    "        visited.add(current_node)\n",
    "        person_info = family(current_node)\n",
    "        person_info[\"distance_from_start\"] = depth  # Annotate with depth\n",
    "        output[current_node] = person_info\n",
    "        \n",
    "        # Add neighbors to the queue with incremented depth\n",
    "        for neighbor in neighbours(person_info):\n",
    "            if neighbor not in visited:\n",
    "                queue.append((neighbor, depth + 1))\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "bfs = BFS('Q317521') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Assuming you already have the BFS result in `bfs_output` as a dictionary of people's information.\n",
    "\n",
    "def generate_family_table(bfs_output):\n",
    "    \"\"\"\n",
    "    Generates a table from BFS output containing family relationships and personal details.\n",
    "    \n",
    "    Args:\n",
    "        bfs_output (dict): Output from the BFS function with personal info.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A table with the family data.\n",
    "    \"\"\"\n",
    "    data = []  # List to hold the table rows\n",
    "    \n",
    "    # Iterate over each person in bfs_output\n",
    "    for person_id, info in bfs_output.items():\n",
    "        # Extract personal information\n",
    "        name = info.get('name', {})\n",
    "        gender = info.get('gender', 'Unknown')\n",
    "\n",
    "        \n",
    "        # Extract relationships\n",
    "        father_id = info.get('father', '')\n",
    "        mother_id = info.get('mother', '')\n",
    "        spouses = info.get('spouses', [])\n",
    "        \n",
    "        # Create family ID (FID) and Mother ID (MID) columns\n",
    "        fid = father_id if father_id else ''\n",
    "        mid = mother_id if mother_id else ''\n",
    "        \n",
    "        # Format the row for this person\n",
    "        row = [\n",
    "            person_id,  # ID\n",
    "            'M' if gender == 'Q6581097' else 'F',  # S (gender M/F)\n",
    "            name,  # Name\n",
    "            fid,  # Father ID\n",
    "            mid,  # Mother ID\n",
    "\n",
    "        ]\n",
    "        \n",
    "        # Add the row to the table\n",
    "        data.append(row)\n",
    "        \n",
    "        # Add spouse relationships (each spouse has a new row in the table)\n",
    "        for spouse in spouses:\n",
    "            # Assuming spouse data is available and structured similarly\n",
    "            spouse_info = bfs_output.get(spouse, {})\n",
    "            spouse_row = [\n",
    "                spouse,  # Spouse ID\n",
    "                'M' if spouse_info.get('gender') == 'Q6581097' else 'F',  # S (gender M/F)\n",
    "                spouse_info.get('name', {}),\n",
    "                spouse_info.get('father', ''),\n",
    "                spouse_info.get('mother', ''),\n",
    "\n",
    "            ]\n",
    "            data.append(spouse_row)\n",
    "    \n",
    "    # Create a pandas DataFrame for the table\n",
    "    columns = [\n",
    "        'ID', 'S', 'Name', 'FID', 'MID'\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "def tree(ID):\n",
    "    ID = next(search('Elon musk'))\n",
    "    bfs = BFS(ID) \n",
    "    rawdf = generate_family_table(bfs)\n",
    "\n",
    "\n",
    "    el1 = rawdf[['ID','MID']]\n",
    "    el2 = rawdf[['ID','FID']]\n",
    "    el1.columns = ['Child', 'ParentID']\n",
    "    el2.columns = el1.columns\n",
    "    el = pd.concat([el1, el2])\n",
    "    el.replace('', np.nan, regex=True, inplace = True)\n",
    "    t = pd.DataFrame({'tmp':['no_entry'+str(i) for i in range(el.shape[0])]})\n",
    "    el['ParentID'].fillna(t['tmp'], inplace=True)\n",
    "    df = el.merge(rawdf, left_index=True, right_index=True, how='left')\n",
    "    df = df.drop(['Child','FID', 'MID'], axis=1)\n",
    "    df = df[['ID', 'Name', 'S',  'ParentID']]\n",
    "\n",
    "    from graphviz import Digraph\n",
    "    f = Digraph('neato', format='jpg', encoding='utf8', filename='corleone', \n",
    "                node_attr={'style': 'filled'},  \n",
    "                graph_attr={\"concentrate\": \"true\", \"splines\":\"ortho\"})\n",
    "    f.attr('node', shape='box')\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Handle cases where 'Name' might be a dictionary or complex object\n",
    "        name = row['Name']\n",
    "        if isinstance(name, dict):  # Extract string value from the dictionary\n",
    "            name = keyword(df['ID'].iloc[index])  # Replace 'value' with the actual key holding the name, if necessary\n",
    "        elif not isinstance(name, str):  # Fallback in case it's not a string\n",
    "            name = str(name)\n",
    "\n",
    "        f.node(row['ID'],\n",
    "            label=name,\n",
    "            _attributes={'color': 'lightpink' if row['S'] == 'F' else 'lightblue' if row['S'] == 'M' else 'lightgray'})\n",
    "    \n",
    "\n",
    "\n",
    "    # Add edges, skipping no_entry ParentID\n",
    "    for index, row in df.iterrows():\n",
    "        if not row[\"ParentID\"].startswith(\"no_entry\"):  # Skip edges with no_entry as ParentID\n",
    "            f.edge(str(row[\"ParentID\"]), str(row[\"ID\"]), label='')  \n",
    "\n",
    "    f.view()\n",
    "\n",
    "\n",
    "\n",
    "tree('Elon musk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = Digraph('neato', format='jpg', encoding='utf8', filename='corleone', node_attr={'style': 'filled'},  graph_attr={\"concentrate\": \"true\", \"splines\":\"ortho\"})\n",
    "#f.attr('node', shape='box')\n",
    "#for index, row in df.iterrows():\n",
    "#    f.node(row['ID'],\n",
    "#           label=\n",
    "#             row['Name'],\n",
    "#           _attributes={'color':'lightpink' if row['S']=='F' else 'lightblue'if row['S']=='M' else 'lightgray'})\n",
    "#for index, row in df.iterrows():\n",
    "#    f.edge(str(row[\"ParentID\"]), str(row[\"ID\"]), label='')  \n",
    "#f.view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
